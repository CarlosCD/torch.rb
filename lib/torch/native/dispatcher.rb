# We use a generic interface for methods (*args, **options)
# and this class to determine the C++ method to call
#
# This is needed since LibTorch uses function overloading,
# which isn't available in Ruby or Python
#
# PyTorch uses this approach, but the parser/dispatcher is written in C++
#
# We could generate Ruby methods directly, but an advantage of this approach is
# arguments and keyword arguments can be used interchangably like in Python,
# making it easier to port code

require "yaml"
require "torch/native/function"

module Torch
  module Native
    module Dispatcher
      class << self
        def bind
          functions = grouped_functions
          bind_functions(::Torch, :define_singleton_method, functions[:torch])
          bind_functions(::Torch::Tensor, :define_method, functions[:tensor])
          bind_functions(::Torch::NN, :define_singleton_method, functions[:nn])
        end

        def bind_functions(context, def_method, functions)
          functions.group_by(&:ruby_name).sort_by { |g, _| g }.each do |name, funcs|
            if def_method == :define_method
              funcs.map! { |f| f.dup }
              funcs.each { |f| f.args.delete("self") }
            end

            defined = def_method == :define_method ? context.method_defined?(name) : context.respond_to?(name)
            next if defined && name != "clone"

            context.send(def_method, name) do |*args, **options|
              matches = funcs.map { |f| f.match(args, options) }.compact
              if matches.size == 1
                send(*matches.first)
              elsif matches.size == 0
                raise ArgumentError, "#{name} received an invalid combination of arguments"
              else
                raise Error, "This should never happen. Please report a bug."
              end
            end
          end
        end

        def generate_cpp_functions
          functions = grouped_functions
          generate_cpp_file("torch", :define_singleton_method, functions[:torch])
          generate_cpp_file("tensor", :define_method, functions[:tensor])
          generate_cpp_file("nn", :define_singleton_method, functions[:nn])
        end

        def generate_cpp_file(type, def_method, functions)
          template = <<-TEMPLATE
// generated by rake generate:functions
// do not edit by hand

#include <torch/torch.h>
#include <rice/Module.hpp>
#include "templates.hpp"

void add_%{type}_functions(Module m) {
  m
  %{functions};
}
        TEMPLATE

          cpp_defs = []
          functions.sort_by(&:cpp_name).each do |func|

            # TODO improve
            cpp_args_str = func.args_str.dup
            cpp_args_str.gsub!("Tensor ", "const Tensor &")
            cpp_args_str.gsub!(/Tensor\(\S!?\) /, "Tensor &")
            cpp_args_str.gsub!("int ", "int64_t ")
            cpp_args_str.gsub!("float ", "double ")
            cpp_args_str.sub!(" *,", "")
            cpp_args_str.gsub!("int[]", "IntArrayRef")
            cpp_args_str.gsub!(/=[^),]+/, "")
            cpp_args_str.gsub!("int64_t reduction", "MyReduction reduction")

            dispatch = func.out? ? "#{func.base_name}_out" : func.base_name
            args = func.args
            args.unshift(*args.pop(func.out_size)) if func.out?
            args.delete("self") if def_method == :define_method

            prefix = def_method == :define_method ? "self." : "torch::"

            cpp_defs << ".#{def_method}(
    \"#{func.cpp_name}\",
    *[](#{cpp_args_str}) {
      return #{prefix}#{dispatch}(#{args.join(", ")});
    })"
          end

          path = File.expand_path("../../../ext/torch/#{type}_functions.cpp", __dir__)
          File.write(path, template % {type: type, functions: cpp_defs.join("\n  ")})
        end

        private

        def functions
          @native_functions ||= YAML.load_file(path).map { |f| Function.new(f) }
        end

        def path
          File.expand_path("native_functions.yaml", __dir__)
        end

        def grouped_functions
          functions = functions()

          # remove functions
          skip_binding = ["unique_dim_consecutive"]
          skip_args = ["?", "[", "Dimname", "ScalarType", "MemoryFormat", "Storage", "ConstQuantizerPtr"]
          functions.reject! { |f| f.ruby_name.start_with?("_") }
          functions.reject! { |f| f.ruby_name.end_with?("_backward") }
          functions.reject! { |f| skip_binding.include?(f.ruby_name) }
          functions.reject! { |f| skip_args.any? { |v| f.args_str.include?(v) } }

          nn_functions, other_functions = functions.partition { |f| f.python_module == "nn" }
          torch_functions = other_functions.select { |f| f.variants.include?("function") }
          tensor_functions = other_functions.select { |f| f.variants.include?("method") }

          {torch: torch_functions, tensor: tensor_functions, nn: nn_functions}
        end
      end
    end
  end
end

# Torch::Native::Dispatcher.bind
